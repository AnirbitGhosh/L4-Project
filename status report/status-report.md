
## *Prediction of overall survival times for breast cancer from H&E stained whole slide biopsy images* 
#### *Anirbit Ghosh* 
#### *2439281g* 

## Proposal
### Motivation
Traditional methods of diagnosing survival times for cancer patients involve collecting a lot of clinical data which is time and resource intesnsive. Cancer biopsy images are also manualy inspected by pathologists to extract data on the sub-cellular structure of the cancerous tissue to supplement the clinical data. Manually extracting this information from cancerous tissue slides is extremely challenging due to the size and complexity of whole slide images. Even trained pathologists can take a very long time to manually annotate a massive whole slide image. This data, however, can be analysed to extract useful quantitative features to be used for a much quicker and potentially more accurate survival time prognoses over bulky clinical data. Thus, Deep Learning provides an exponentially quicker solution to this problem by allowing us to automatically segment and detect cancerous tissue from WSIs using a model that has been trained on existing pathologist annotations. Using Deep Learning to automatically detect cancerous tissue from WSIs allow us to employ image data to quantify histopathological features and make much faster survival time prognoses.

### Aims
The aim of the project is to train a Deep Convolutional Neural Network using pre-annotated breast cancer slide images to produce a supervised learning model that can detect cancerous tissue from any given breast tissue biopsy image. Once the cancerous section has been identified in the given biopsy image by the model, image processing techniques will be employed to extract useful features to be used as a physical quanitifier of the severity of the patient's cancer. These extracted features will be used as covariates in developing a regression model fitted using survival time data of patients publicly available on TCGA/CBioPortal to investigate the association between the physical histopathological characteristics of malignant tissue and the survival times of patients. The effectiveness of the model will be determined by using our model to predict survival times of unseen patients and validating against the available clinical ground truth data. 

## Progress
* Language and version control framework setup - Python Pytorch for Deep Learning and statistical analysis and Github for VC.
* Training, testing and validation data acquired from Camelyon16 Grand Challenge database - PCAM dataset - annotated breast cancer lymph node tissue samples, that was pre-tiled into 96x96px patches and split into train/test/validation datasets by pathologists.
* Build database for survival modelling using publicly available clinical data + biopsy slide images from TCGA portal - extract all patients with available survival data on CBioPortal and gather each patient's corresponding biopsy sample from TCGA (75 total WSIs gathered). 
* Implement tiling and normalization of these Whole Slide images - WSIs from TCGA patient database are first tiled into 96x96px patches and then colour normalized to eliminate any staining differences between samples. 
* Create and train DCNN using full PCAM training dataset of 100k normalized WSI tiles
* Save trained network weights
* Use trained network on every tiled WSI to generate a full malignancy prediction report  - Prediction is recorded at a tile level for each 96x96px tile of the WSI. Prediction is a probability value between 0 and 1, indicating likelihood of the tile being a malignant patch. Malignancy determined by having at least one malignant cell in the central 32x32px area of each tile. 
* Map malignancy likelihood of each 96x96px tile to the whole slide image and build a heatmap using a continuous colour map to indicate dominant malignant regions.
* Using malignancy likelihoods, extract number of tiles with a likelihood >= 0.7, quantifying malignant regions of high certainty. Convert this number of malignant tiles into a percentage of total tiles, converting it into a malignancy score. Also extract mean intensity of each malignancy heatmap, higher intensities indicating more areas of high likelihood malignant tissue. 
* Use extracted features from whole slide images to fit a Cox-Proportional Hazard model with two covariates. Use survival time (in months) from CBioPortal for each patient whose WSI was processed, combined with the malignancy score and mean intensity of malignancy heatmap as covariates to generate a survival model. Plot Kaplan Meier plots of the fitted CPH model and test on various levels of malignancy score or mean intensities to observe the association between survival times and features extracted from whole slide images. 
  


## Problems and risks
### Problems
* I initially ran into a problem surrounding data availability to train a DCNN for cancer detection. All literature in this domain manually annotated WSIs and built their own training dataset which was not possible in this case as we do not have any trained pathologists to do the annotations for us. All data publicly available was without annotations, so creating a supervised learning model was turning out to be difficult. Finally used the CAMELYON16 Grand Challenge dataset, which is a publicly available, fully-annotated breast cancer dataset. PCAM is a version of this dataset which tiles every WSI and associates a label (0: benign, 1: malignant) to each tile based on its clinical annotation. 
* Ran into a problem regarding memory and storage constraints when trying to process and store Whole Slide Images. The training and validation dataset for our model contained a 150k tiles. This was compressed by PCAM developers so it did not cause a problem there. However, our final dataset to be used for survival time modelling involved downloading WSIs from TCGA and then analysing each WSI. There was a significant number of WSIs needed to be able to generate a reasonable survival model, we managed to acquire 75 WSIs with available clinical data about the patient's survival time. These WSIs were 1.5GB to 2GB in size each. Resulting in the whole dataset of 75 WSIs being over 200gb. Subsequently, we needed to tile every WSI into 96x96px patches as our model was trained on 96x96px tiles. Splitting a WSI into 96x96px tiles at a 10x downsampled magnification resulted in producing over 70,000-95,000 tiles for each WSI. Now all the tiled WSIs added up to over 900GB of image data which I physically could not store or process on my own computer. It would take days to iterate over 6 million tiles and that is not feasible as the whole processing needs to be done several times while trying to develop a better model. Finally, the solution was to use my supervisor's Linux workstation to store the full dataset (training data of model and diagnostic dataset of 75 tiled WSIs containing 6million tiles). I initially trained the DCNN on my personal machine and saved the weights of the best model. Subsequently, I used SSH to connect to the Linux machine to pass all 6million tiles through the network to generate prediction data and finally combine all prediction data to render 75 malignancy heatmaps. 


### Risks
* A potential risk I encountered was the fact that there might not have been any real correlation between the biopsy image of cancerous tissue and the patient's survival time at all. It could be possible for the survival time to be entirely influenced by other clinical factors like age, smoking habits, underlying medial conditions etc. and not by the severity of the cancer alone. However, after several iterations of training and optimizing the DCNN and changing what image features are extracted, I was able to arrive at a model which showed statistically significant covariance on survival time when changing the malignancy score indicating this image feature does in fact influence survival time and can generate approximate survival time prognoses. 
* Due to the absence of any data at all in this field, it was necessary to use the pre-annotated data from CAMELYON16. However, this data is of breast metastasis found in lymph node tissue and not breast tissue itself. Whereas, the TCGA data used to build the diagnostic dataset for survival modelling is entirely composed of breast tissue samples. There was a risk of the model trained on lymph node tissue not translating to breast tissue and completely failing to produce any meaningful predictions at all. However, after actually training and optimizing the model, it was able to generate very good predictions on the breast tissue samples. This indicated that breast metastasis found in lymph node tissue is in fact representative of the metastasis found in breast tissue itself and can be used to accurately predict breast cancer. 

## Plan
The majority of the coding side of the project is complete. Trained DCNN model producing good results. The model predictions produced are generating reasonable survival time predictions. Thus the plan for Semster 2 is as follows: 
* Jan-Feb: Start working on writing up the dissertation. A lot of background information has to be elaborated upon. All the models must be run again to collect data samples and visualization samples to be used in the dissertation to describe the workflow of reaching our final models. While writing the background and other research, more ideas might come up that can improve the models even further, apply those changes and fine tune the core models that are already complete. **End of January: prepare a full first draft of the  dissertation. Submit draft to supervisor for initial feedback**
* Feb-March: Final touches to disseration and produced model. Clean up and polish code repo for reproducibility and submission. **End of March: Prepare for final submission of dissertation and source code**


